{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick tour (Group A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This quick tour will help you get started with ```transformers``` library.  It will show you how to load preprocessors,i.e. tokenizers, and language models with an [transformers.AutoClass](https://huggingface.co/docs/transformers/main/en/./model_doc/auto), and quickly train and evaluate a model with [transformers.Trainer](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer). \n",
    "* You only need to read what is in this notebook as some hyper links are only for reference use. \n",
    "* You will need to run each cell of code in order.\n",
    "* You can ask the instructor to clarify any concept that you feel unclear in this tutorial.\n",
    "* **This tutorial can be referred back to during the later assignment, so please read this notebook carefully.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### You have 15 minutes on this tutorial. Let the instructor start timing when you read this sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Library Import (run the code, no need to read through it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# no need to read the import block\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n",
    "os.environ['HF_HOME'] = '/workspace/HF_cache/'\n",
    "os.environ['HF_DATASETS_CACHE'] = '/workspace/HF_cache/datasets'\n",
    "os.environ['TRANSFORMERS_CACHE'] = '/workspace/HF_cache/transformers_cache/'\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS']='0'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1' \n",
    "\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Natural Language Processing via the Transformers Library "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An NLP model takes tokenized text as input and outputs numerical values to solve common NLP tasks, with some examples of each:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "</Tip>\n",
    "\n",
    "| **Task**                     | **Description**                                                                                              | **Application** |\n",
    "|------------------------------|--------------------------------------------------------------------------------------------------------------|------------------------------|\n",
    "| Masked language modeling  | predicts a masked token in a sequence                                                                                 | pre-training |          \n",
    "| Sequence classification          | assign a label to a given sequence of text                                                                   | sentiment analysis |  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ```transformers``` library provides the functionality and API to create and use such NLP models. And these models can be stored locally in the file system. The ```transformers``` library provides a unified API with few user-facing abstractions for using NLP models so that it has low barrier to entry for educators and practitioners."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The code below shows you an example that there are three models stored locally under ```models```. \n",
    "* Among the models,  ```models/bert-base-uncased_v2``` and ```models/bert-base-uncased-sentiment``` are derived from ```models/bert-base-uncased```.\n",
    "* Specifically, ```models/bert-base-uncased_v2``` is the next version of ```models/bert-base-uncased``` via fine-tuning and they perform the same pre-training task, i.e.  masked language modeling.\n",
    "* ```models/bert-base-uncased-sentiment``` is adapted from ```models/bert-base-uncased``` and is trained to perform a downstream task, i.e. sequence classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config.json\t\tspecial_tokens_map.json  training_args.bin\r\n",
      "generation_config.json\ttokenizer.json\t\t vocab.txt\r\n",
      "pytorch_model.bin\ttokenizer_config.json\r\n"
     ]
    }
   ],
   "source": [
    "!ls models/bert-base-uncased # this model can perform task ```Masked language modeling```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config.json\t\tspecial_tokens_map.json  training_args.bin\r\n",
      "generation_config.json\ttokenizer.json\t\t vocab.txt\r\n",
      "pytorch_model.bin\ttokenizer_config.json\r\n"
     ]
    }
   ],
   "source": [
    "!ls models/bert-base-uncased_v2 # this model can perform task ```Masked language modeling```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config.json\t   special_tokens_map.json  tokenizer_config.json  vocab.txt\r\n",
      "pytorch_model.bin  tokenizer.json\t    training_args.bin\r\n"
     ]
    }
   ],
   "source": [
    "!ls models/bert-base-uncased-sentiment # this model can perform task ```Sequence classification```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Loading Tokenizer\n",
    "\n",
    "A tokenizer is responsible for preprocessing text into an array of numbers as inputs to a model. The most important thing to remember is you need to instantiate a tokenizer with the same model name to ensure you're using the same tokenization rules a model was pretrained with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_name = \"models/bert-base-uncased-sentiment\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pass your text to the tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 11312, 10320, 12495, 19308, 10114, 11391, 10855, 10103, 100, 58263, 13299, 119, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "encoding = tokenizer(\"We are very happy to show you the ðŸ¤— Transformers library.\")\n",
    "print(encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tokenizer returns a dictionary containing:\n",
    "\n",
    "* [input_ids](https://huggingface.co/docs/transformers/main/en/./glossary#input-ids): numerical representations of your tokens.\n",
    "* [attention_mask](https://huggingface.co/docs/transformers/main/en/.glossary#attention-mask): indicates which tokens should be attended to.\n",
    "\n",
    "A tokenizer can also accept a list of inputs, and pad and truncate the text to return a batch with uniform length:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_batch = tokenizer(\n",
    "    [\"We are very happy to show you the Transformers library.\", \"We hope you don't hate it.\"],\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=512,\n",
    "    return_tensors=\"pt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Loading Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```transformers``` provides a simple and unified way to load different instances. This means you can load an [AutoModel](https://huggingface.co/docs/transformers/main/en/model_doc/auto#transformers.AutoModel) like you would load an [AutoTokenizer](https://huggingface.co/docs/transformers/main/en/model_doc/auto#transformers.AutoTokenizer). The only difference is selecting the correct [AutoModel](https://huggingface.co/docs/transformers/main/en/model_doc/auto#transformers.AutoModel) for the task which can be achieved by either calling the specific class, i.e. AutoModelForSequenceClassification in this exampe, or calling AutoConfig."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model_name = \"models/bert-base-uncased-sentiment\"\n",
    "model1 = AutoModelForSequenceClassification.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig\n",
    "\n",
    "config = AutoConfig.from_pretrained(model_name)\n",
    "architecture = config.architectures[0]\n",
    "model2 = getattr(transformers, architecture).from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "</Tip>\n",
    "\n",
    "Now pass your preprocessed batch of inputs directly to the model. You just have to unpack the dictionary by adding `**`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.equal(model1(**pt_batch).logits, model2(**pt_batch).logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All models are a standard [`torch.nn.Module`](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) so you can use them in any typical training loop. While you can write your own training loop, ```transformers``` provides a [Trainer](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer) class for PyTorch, which contains the basic training loop and adds additional functionality for features like distributed training, mixed precision, and more.\n",
    "\n",
    "Depending on your task, you'll typically pass the following parameters to [Trainer](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer):\n",
    "\n",
    "1. A [PreTrainedModel](https://huggingface.co/docs/transformers/main/en/main_classes/model#transformers.PreTrainedModel) or a [`torch.nn.Module`](https://pytorch.org/docs/stable/nn.html#torch.nn.Module):\n",
    "\n",
    "   ```py\n",
    "   >>> from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "   >>> model = AutoModelForSequenceClassification.from_pretrained(\"models/bert-base-uncased-sentiment\")\n",
    "   ```\n",
    "\n",
    "2. [TrainingArguments](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.TrainingArguments) contains the model hyperparameters you can change like learning rate, batch size, and the number of epochs to train for. The default values are used if you don't specify any training arguments:\n",
    "\n",
    "   ```py\n",
    "   >>> from transformers import TrainingArguments\n",
    "\n",
    "   >>> training_args = TrainingArguments(\n",
    "   ...     output_dir=\"tmp_trainer\",\n",
    "   ...     per_device_train_batch_size=256,\n",
    "   ...     per_device_eval_batch_size=256,\n",
    "   ... )\n",
    "   ```\n",
    "\n",
    "3. A preprocessing class like a tokenizer, image processor, feature extractor, or processor:\n",
    "\n",
    "   ```py\n",
    "   >>> from transformers import AutoTokenizer\n",
    "\n",
    "   >>> tokenizer = AutoTokenizer.from_pretrained(\"models/bert-base-uncased-sentiment\")\n",
    "   ```\n",
    "\n",
    "4. Load a dataset:\n",
    "\n",
    "   ```py\n",
    "   >>> from datasets import load_dataset\n",
    "\n",
    "   >>> dataset = load_dataset(\"datasets/rotten_tomatoes\")\n",
    "   ```\n",
    "\n",
    "5. Create a function to tokenize and preprocess the dataset:\n",
    "\n",
    "   ```py\n",
    "   >>> def preprocess_dataset(dataset):\n",
    "   ...     return tokenizer(dataset[\"text\"])\n",
    "   ```\n",
    "\n",
    "   Then apply it over the entire dataset with [map](https://huggingface.co/docs/datasets/main/en/package_reference/main_classes#datasets.Dataset.map):\n",
    "\n",
    "   ```py\n",
    "   >>> dataset = dataset.map(preprocess_dataset, batched=True)\n",
    "   ```\n",
    "\n",
    "6. Now gather all these classes in [Trainer](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer):\n",
    "\n",
    "   ```py\n",
    "   >>> from transformers import Trainer\n",
    "   >>> trainer = Trainer(\n",
    "   ...      model=model,\n",
    "   ...      args=training_args,\n",
    "   ...      train_dataset=dataset[\"train\"],\n",
    "   ...      eval_dataset=dataset[\"test\"],\n",
    "   ...      tokenizer=tokenizer,\n",
    "   ...)\n",
    "   ```\n",
    "7. Call [trainer.train()](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer.train) to start training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## * Useful Note:\n",
    "```transformers``` lets the user to derive a model that can perform a downstream task, e.g. sequence classification, from a pre-training model, e.g. to produce a sequence classification model like ```models/bert-base-uncased-sentiment``` from a pretrained masked language model ```models/bert-base-uncased_v2```, the user can simply do: \n",
    "\n",
    "```py\n",
    "   >>> model = AutoModelForSequenceClassification.from_pretrained(\"models/bert-base-uncased_v2\") # models/bert-base-uncased_v2's weight will be automatically loaded into the sequence classification model with some additional weights that are randomly initiated for the downstream task\n",
    "   >>> trainer = Trainer(\n",
    "   ...    model=model,\n",
    "   ...    args=training_args,\n",
    "   ...    train_dataset=dataset[\"train\"],\n",
    "   ...    eval_dataset=dataset[\"test\"],\n",
    "   ...    tokenizer=tokenizer,\n",
    "   ...)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Trainer](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer) does not automatically evaluate model performance during training. You'll need to pass [Trainer](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer) a function to compute and report metrics. The [Evaluate](https://huggingface.co/docs/evaluate/index) library provides a simple [`accuracy`](https://huggingface.co/spaces/evaluate-metric/accuracy) function you can load with the [evaluate.load](https://huggingface.co/docs/evaluate/main/en/package_reference/loading_methods#evaluate.load) function:\n",
    "\n",
    "```py\n",
    "    >>> import numpy as np\n",
    "    >>> import evaluate\n",
    "\n",
    "    >>> metric = evaluate.load(\"accuracy\")\n",
    "```\n",
    "\n",
    "2. Call `compute` on `metric` to calculate the accuracy of your predictions. Before passing your predictions to `compute`, you need to convert the predictions to logits (remember all ```Transformers``` models return logits):\n",
    "\n",
    "```py\n",
    "    >>> def compute_metrics(eval_pred):\n",
    "    ...    logits, labels = eval_pred\n",
    "    ...    predictions = np.argmax(logits, axis=-1)\n",
    "    ...    return metric.compute(predictions=predictions, references=labels)\n",
    "```\n",
    "\n",
    "3. Now gather all these classes in [Trainer](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer):\n",
    "\n",
    "```py\n",
    "    >>> from transformers import TrainingArguments, Trainer\n",
    "\n",
    "    >>> training_args = transformers.TrainingArguments(output_dir=os.path.join(\"tmp_trainer\"),)\n",
    "    >>> trainer = transformers.Trainer(\n",
    "    ...      model=model,\n",
    "    ...      args=training_args,\n",
    "    ...      tokenizer=tokenizer,\n",
    "    ...      eval_dataset=dataset,\n",
    "    ...      compute_metrics=compute_metrics,\n",
    "    ...)\n",
    "```\n",
    "4. Call [trainer.evaluate()](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer.evaluate) to start evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Don't close this tab when you are done reading!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
