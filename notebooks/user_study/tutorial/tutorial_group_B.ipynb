{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick tour (Group B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This quick tour will help you get started with ```mgit``` library and the concept of lineage graph. It will show you how to load preprocessors,i.e. tokenizers, and language models with mgit, and quickly train and evaluate the model. \n",
    "* You only need to read what is in this notebook as some hyper links are only for reference use. \n",
    "* You will need to run each cell of code in order.\n",
    "* You can ask the instructor to clarify any concept that you feel unclear in this tutorial.\n",
    "* **This tutorial can be referred back to during the later assignment, so please read this notebook carefully.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  You have 15 minutes on this tutorial. Let the instructor start timing when you read this sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Library Import (run the code, no need to read through it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# no need to read the import block\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n",
    "os.environ['HF_HOME'] = '/workspace/HF_cache/'\n",
    "os.environ['HF_DATASETS_CACHE'] = '/workspace/HF_cache/datasets'\n",
    "os.environ['TRANSFORMERS_CACHE'] = '/workspace/HF_cache/transformers_cache/'\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS']='0'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1' \n",
    "\n",
    "import sys\n",
    "MGIT_PATH=os.path.dirname(os.path.dirname(os.path.dirname(os.getcwd())))\n",
    "sys.path.append(MGIT_PATH)\n",
    "from utils.lineage.graph import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Natural Language Processing via the MGit Library "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An NLP model takes tokenized text as input and outputs numerical values to solve common NLP tasks, with some examples of each:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "</Tip>\n",
    "\n",
    "| **Task**                     | **Description**                                                                                              | **Application** |\n",
    "|------------------------------|--------------------------------------------------------------------------------------------------------------|------------------------------|\n",
    "| Masked language modeling  | predicts a masked token in a sequence                                                                                 | pre-training |          \n",
    "| Sequence classification          | assign a label to a given sequence of text                                                                   | downstream: sentiment analysis |   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ```mgit``` library provides the functionality and API to create and use such NLP models. The ```mgit``` library also provides a data structure ```LineageGraph``` to store and manage the models by recording their lineage relations, i.e. how a model is derived from/related to other models in the graph. **This data structure lets users efficiently retrieve, inspect and update models which show similar behavior because of their resemblance (e.g., similar layers).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The code below shows you an example lineage graph that stores three models locally under ```models```. \n",
    "* Among the models,  ```models/bert-base-uncased_v2``` and ```models/bert-base-uncased-sentiment``` are derived from ```models/bert-base-uncased```.\n",
    "* Specifically, ```models/bert-base-uncased_v2``` is the next version of ```models/bert-base-uncased``` via fine-tuning and they perform the same pre-training task, i.e.  masked language modeling.\n",
    "* ```models/bert-base-uncased-sentiment``` is adapted from ```models/bert-base-uncased``` and is trained to perform a downstream task, i.e. sequence classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the LineageGraph\n",
    "g = LineageGraph.load_from_file('./')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1000\"\n",
       "            height=\"450\"\n",
       "            src=\"LineageGraph.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fbc52cf4040>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# show the graph with interactive mode where you can drag the node/graph and zoom in/out \n",
    "from IPython.display import IFrame\n",
    "g.show()\n",
    "display(IFrame('LineageGraph.html', width=1000, height=450))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see all the models in the LineageGraph ```g``` you can do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/bert-base-uncased\n",
      "models/bert-base-uncased_v2\n",
      "models/bert-base-uncased-sentiment\n"
     ]
    }
   ],
   "source": [
    "for node in g.get_all_nodes():\n",
    "    print(node.output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To see all the models in the subtree of a specific parent model, i.e. models that are originated from a model, you can pass in the name of the parent. This is useful when users want to quickly retrieve other models which might show similar behavior, e.g., accuracy fluctuations due to subtle changes in inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/bert-base-uncased_v2\n",
      "models/bert-base-uncased-sentiment\n"
     ]
    }
   ],
   "source": [
    "# Here is an example to retrive the subtree of 'models/bert-base-uncased':\n",
    "for node in g.get_all_nodes(parent='models/bert-base-uncased'):\n",
    "    print(node.output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Loading Tokenizer\n",
    "\n",
    "A tokenizer is responsible for preprocessing text into an array of numbers as inputs to a model. The most important thing to remember is you need to instantiate a tokenizer with the same model name to ensure you're using the same tokenization rules a model was pretrained with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading model: models/bert-base-uncased-sentiment\n",
      "attempting to load model by config\n"
     ]
    }
   ],
   "source": [
    "#Each model is stored as a lineag node inside the lineage Graph, where the model instance and the tokenizer instance can be retrieved from.\n",
    "tokenizer = g.get_node(\"models/bert-base-uncased-sentiment\").get_pt_tokenizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pass your text to the tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 11312, 10320, 12495, 19308, 10114, 11391, 10855, 10103, 18437, 10517, 13299, 119, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "encoding = tokenizer(\"We are very happy to show you the mgit library.\")\n",
    "print(encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tokenizer returns a dictionary containing:\n",
    "\n",
    "* [input_ids](https://huggingface.co/docs/transformers/main/en/./glossary#input-ids): numerical representations of your tokens.\n",
    "* [attention_mask](https://huggingface.co/docs/transformers/main/en/.glossary#attention-mask): indicates which tokens should be attended to.\n",
    "\n",
    "A tokenizer can also accept a list of inputs, and pad and truncate the text to return a batch with uniform length:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_batch = tokenizer(\n",
    "    [\"We are very happy to show you the mgit library.\", \"We hope you don't hate it.\"],\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=512,\n",
    "    return_tensors=\"pt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Loading Model \n",
    "\n",
    "As stated above, ```mgit``` provides a simple and unified way to load different instances. This means you can also retrieve a model instance from a ```LineageNode```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = g.get_node(\"models/bert-base-uncased-sentiment\").get_pt_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "</Tip>\n",
    "\n",
    "Now pass your preprocessed batch of inputs directly to the model. You just have to unpack the dictionary by adding `**`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.6026, -2.7280, -0.7415,  2.0235,  3.1213],\n",
       "        [ 0.0064, -0.1258, -0.0503, -0.1655,  0.1329]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(**pt_batch).logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All models are a standard [`torch.nn.Module`](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) so you can use them in any typical training loop. While you can write your own training loop, ```mgit``` provides a ```LineageTrain``` class for PyTorch, which contains the basic training loop and adds additional functionality for features like distributed training, mixed precision, and more.\n",
    "\n",
    "Depending on your task, you'll typically pass the following parameters to use LineageTrain:\n",
    "\n",
    "1. A LineagNode added to the loaded graph:\n",
    "   ```py\n",
    "   \n",
    "   >>> node = LineageNode(\n",
    "                    init_checkpoint='models/bert-base-uncased-sentiment',\n",
    "                    output_dir='models/bert-base-uncased-sentiment_versioned',\n",
    "                )\n",
    "   >>> g.add(node, etype='adapted',parent='models/bert-base-uncased-sentiment')\n",
    "   ```\n",
    "3. A preprocessing class like a tokenizer, image processor, feature extractor, or processor:\n",
    "\n",
    "   ```py\n",
    "   >>> tokenizer = node.get_pt_tokenizer()\n",
    "   ```\n",
    "   \n",
    "4. A ```LineageDataset```:\n",
    "\n",
    "   ```py\n",
    "   >>> lieange_dataset = LineageDataset('datasets/rotten_tomatoes', feature_keys=['text'])\n",
    "   ```\n",
    "\n",
    "5. Create a function to tokenize and preprocess the dataset:\n",
    "\n",
    "   ```py\n",
    "   >>> def preprocess_function(lieange_dataset, tokenizer):\n",
    "           lieange_dataset.dataset = lieange_dataset.dataset.map(tokenizer(dataset[\"text\"]), batched=True)\n",
    "           return lieange_dataset\n",
    "   ```\n",
    "\n",
    "   Then apply it over the entire dataset:\n",
    "\n",
    "   ```py\n",
    "   >>> lieange_dataset = preprocess_function(lieange_dataset, tokenizer)\n",
    "   ```\n",
    "\n",
    "6. Now gather all these classes in LineageTrain and add it to the node. LineageTrain contains the model hyperparameters you can change like learning rate, batch size, and the number of epochs to train for. The [default](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.TrainingArguments) values are used if you don't specify any training arguments:\n",
    "\n",
    "```py\n",
    "    >>> lineage_train = LineageTrain(\n",
    "    ...     train_dataset=dataset['train'],\n",
    "    ...     eval_dataset=dataset['test'],\n",
    "    ...     per_device_train_batch_size=256,\n",
    "    ...     per_device_eval_batch_size=256,\n",
    "    ... )\n",
    "    >>> node.lineage_train = lineage_train\n",
    "```\n",
    "7. Call ```node.train()``` to start training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## * Useful Note:\n",
    "```mgit``` also provides an advanced function ```update_cascade``` to enable **efficient training** if the user wants to replay the adaptation process of a downstream model, e.g. sequence classification, **on a different pre-training model**, e.g. masked language modeling. In the above example, ```models/bert-base-uncased-sentiment``` was adapted from ```models/bert-base-uncased```. To create a new version of ```models/bert-base-uncased-sentiment``` from ```models/bert-base-uncased_v2``` instead of ```models/bert-base-uncased```, the user can simply do: \n",
    "```py\n",
    ">>> #old_base is the reference node where the training process to produce its down stream models is recorded by mgit, and new_base is the new pre-training node that the recorded training process will be replayed on.\n",
    ">>> #new_target is then fine-tuned from new_base and is returned as the next version of old_target \n",
    ">>> new_target = g.update_cascade(old_base=g.get_node('models/bert-base-uncased'), new_base=g.get_node('models/bert-base-uncased_v2'), old_target=g.get_node('models/bert-base-uncased-sentiment'))\n",
    "```\n",
    "#### Note that the new_target is automatically named with a suffix '_verisoned' compared to odel_target, to retrieve it from the LineageGraph ```g```, the user can simply do:\n",
    "```py\n",
    ">>> new_target = g.get_node(g.get_node('models/bert-base-uncased-sentiment').output_dir + '_versioned')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    " \n",
    " \n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. ```mgit``` provides a customizable ```LineageTest``` class for model evaluation. You'll need to pass LineageTest a function to compute and report metrics. As an example, the [Evaluate](https://huggingface.co/docs/evaluate/index) library provides a simple [`accuracy`](https://huggingface.co/spaces/evaluate-metric/accuracy) function you can load with the [evaluate.load](https://huggingface.co/docs/evaluate/main/en/package_reference/loading_methods#evaluate.load) function:\n",
    "\n",
    "```py\n",
    "    >>> import numpy as np\n",
    "    >>> import evaluate\n",
    "\n",
    "    >>> metric = evaluate.load(\"accuracy\")\n",
    "```\n",
    "\n",
    "2. Call `compute` on `metric` to calculate the accuracy of your predictions. Before passing your predictions to `compute`, you need to convert the predictions to logits:\n",
    "\n",
    "```py\n",
    "    >>> def compute_metrics(eval_pred):\n",
    "    ...    logits, labels = eval_pred\n",
    "    ...    predictions = np.argmax(logits, axis=-1)\n",
    "    ...    return metric.compute(predictions=predictions, references=labels)\n",
    "```\n",
    "\n",
    "3. Now gather all these classes in LineageTest:\n",
    "\n",
    "```py\n",
    "    >>> test = LineageTest(      \n",
    "    ...    eval_dataset=dataset['test'],\n",
    "    ...    compute_metrics=compute_metrics,\n",
    "    ...    name='test_1',\n",
    "    )\n",
    "```\n",
    "4. Call ```node.run_test(test, return_results=True)``` to start evaluation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Don't close this tab when you are done reading!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
