{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment A (Group B): \n",
    "* In this assignment, you will first learn about a bug in a language model and how to reproduce it.\n",
    "* Given a pool of models, your goal is then to find **as many models as possible** that exhibit this bug, under a 15-minutes time constraint.\n",
    "* This notebook walks you through this process step-by-step. Run each cell of code and read the text instructions untill you read section 5 where you will need to write your own code to find the buggy models.\n",
    "* If you have any question during the assignment, please ask the instructor directly. It is prohibited to consult with any generative language models, e.g. ChatGPT, about this assignment. Please do not search for these bugs on the internet either."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### You are given 15 minutes to finish this assignment. Let the instructor start timing when you read this sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Library Import (run the code, no need to read through it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n",
    "os.environ['HF_HOME'] = '/workspace/HF_cache/'\n",
    "os.environ['HF_DATASETS_CACHE'] = '/workspace/HF_cache/datasets'\n",
    "os.environ['TRANSFORMERS_CACHE'] = '/workspace/HF_cache/transformers_cache/'\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS']='0'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1' \n",
    "import sys\n",
    "MGIT_PATH=os.path.dirname(os.path.dirname(os.path.dirname(os.getcwd())))\n",
    "sys.path.append(MGIT_PATH)\n",
    "from utils.lineage.graph import *\n",
    "from IPython.display import IFrame\n",
    "import torch\n",
    "import time\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Models\n",
    "\n",
    "* The ```models``` directory has 91 language models. You can inspect them using ```!ls models``` later.\n",
    "```\n",
    "TehranNLP-org\t       bert-large-uncased-7        roberta-large-10\n",
    "albert-base-v2\t      bert-large-uncased-8        roberta-large-2\n",
    "aloxatel\t            bert-large-uncased-9        roberta-large-3\n",
    "bert-base-cased         deepset\t\t\t         roberta-large-4\n",
    "bert-base-uncased       distilbert-base-cased-0\t roberta-large-5\n",
    "bert-large-cased-0      distilbert-base-cased-1\t roberta-large-6\n",
    "bert-large-cased-1      distilbert-base-cased-10    roberta-large-7\n",
    "bert-large-cased-10     distilbert-base-cased-2\t roberta-large-8\n",
    "bert-large-cased-2      distilbert-base-cased-3\t roberta-large-9\n",
    "bert-large-cased-3      distilbert-base-cased-4\t roberta-large-mnli-0\n",
    "bert-large-cased-4      distilbert-base-cased-5\t roberta-large-mnli-1\n",
    "bert-large-cased-5      distilbert-base-cased-6\t roberta-large-mnli-10\n",
    "bert-large-cased-6      distilbert-base-cased-7\t roberta-large-mnli-2\n",
    "bert-large-cased-7      distilbert-base-cased-8\t roberta-large-mnli-3\n",
    "bert-large-cased-8      distilbert-base-cased-9\t roberta-large-mnli-4\n",
    "bert-large-cased-9      distilbert-base-uncased\t roberta-large-mnli-5\n",
    "bert-large-uncased-0    doc2query\t\t \t\t  roberta-large-mnli-6\n",
    "bert-large-uncased-1    ericRosello\t\t \t\troberta-large-mnli-7\n",
    "bert-large-uncased-10   google\t\t\t \t\t roberta-large-mnli-8\n",
    "bert-large-uncased-2    howey\t\t\t \t\t  roberta-large-mnli-9\n",
    "bert-large-uncased-3    prajjwal1\t\t \t\t  t5-base\n",
    "bert-large-uncased-4    roberta-base\t\t \t   textattack\n",
    "bert-large-uncased-5    roberta-large-0\t\t \ttwmkn9\n",
    "bert-large-uncased-6    roberta-large-1\t\t \tvennify\n",
    "```\n",
    "* Some folders contain sub-directories with more models. For example, ```models/deepset``` has multiple models within it (e.g.,  ```bert-base-uncased-squad2```  ```roberta-base-squad2```  ```roberta-large-squad2```)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* By using MGit, you can view the lineage relation among the 91 models. You can zoom in to find the model names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"800\"\n",
       "            height=\"450\"\n",
       "            src=\"LineageGraph.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f6f3f268f40>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "g = LineageGraph.load_from_file('./') # load the Lineage graph where the models are stored in\n",
    "g.show()  # output html file and pdf file that shows the lineage relations between models\n",
    "display(IFrame('LineageGraph.html', width=800, height=450)) # dispaly html file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Here is an example to get the node storing model ```models/t5-base```. Note that ```models/t5-base``` is the root of the rightmost subgraph in the above LineageGraph ```g```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "node = g.get_node('models/t5-base')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the code to load a dataset. You do not need to understand the details here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "from functools import partial\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchtext.datasets import CNNDM\n",
    "\n",
    "cnndm_batch_size = 64\n",
    "cnndm_datapipe = CNNDM(split=\"test\")\n",
    "task = \"summarize\"\n",
    "\n",
    "def apply_prefix(task, x):\n",
    "    return f\"{task}: \" + x[0], x[1]\n",
    "\n",
    "cnndm_datapipe = cnndm_datapipe.map(partial(apply_prefix, task))\n",
    "cnndm_datapipe = cnndm_datapipe.batch(cnndm_batch_size)\n",
    "cnndm_datapipe = cnndm_datapipe.rows2columnar([\"article\", \"abstract\"])\n",
    "cnndm_dataloader = DataLoader(cnndm_datapipe, shuffle=False, batch_size=None)\n",
    "                              \n",
    "lineage_dataset = LineageDataset(dataset=cnndm_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Bug Behavior: The Model Outputs NaN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* If a model outputs NaN (Not a Number), it means the model contains a bug.\n",
    "* Your colleague finds that the ```models/t5-base``` model, when loaded in torch.fp16 format (using ```model.half()```), returns NaN in its output.\n",
    "* Your colleague wrote a test function to compute the NaN rate, i.e. percentage of outputs that contain NaN when running on the entire given dataset. The NaN rate for ```models/t5-base``` is greater than 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the code; you do not need to understand the exact details here\n",
    "\n",
    "# Here we define the test function\n",
    "def custom_test_function(model, lineage_dataset, tokenizer):\n",
    "    fixed_input_length = 128\n",
    "    model.half()\n",
    "    model.to(\"cuda:0\")\n",
    "    model.eval()\n",
    "    \n",
    "    nan = 0\n",
    "    j = 0\n",
    "    decoder_input_ids = torch.tensor([[tokenizer.pad_token_id for n in range(fixed_input_length)] for m in range(cnndm_batch_size)]).to(\"cuda:0\")\n",
    "    total = sum(1 for e in lineage_dataset.dataset) - 1 #drop last\n",
    "\n",
    "    for batch in tqdm(lineage_dataset.dataset, total=total):\n",
    "        input_text = batch[\"article\"]\n",
    "\n",
    "        if j == total:#drop last\n",
    "            break\n",
    "\n",
    "        inputs = tokenizer(input_text, max_length=fixed_input_length, padding=True, truncation=True, return_tensors=\"pt\").to(\"cuda:0\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            try:\n",
    "                out = model(**inputs, decoder_input_ids=decoder_input_ids)\n",
    "            except Exception as e:\n",
    "                out = model(**inputs)\n",
    "                \n",
    "        try:\n",
    "            if hasattr(out, 'last_hidden_state'):\n",
    "                nan += sum([torch.isnan(out_).any() for out_ in out.last_hidden_state])\n",
    "            elif hasattr(out, 'logits'):\n",
    "                nan += sum([torch.isnan(out_).any() for out_ in out.logits])\n",
    "            else:\n",
    "                nan += sum([torch.isnan(out.start_logits[i]).any() and torch.isnan(out.end_logits[i]).any()\n",
    "                            for i in range(len(out.start_logits))])\n",
    "        except Exception as e:\n",
    "            print(e, \"model output enterpretation is unsuccessful!\")\n",
    "            model.to('cpu')\n",
    "            return {'nan_rate': nan/(cnndm_batch_size*total)}\n",
    "        \n",
    "        j += 1\n",
    "\n",
    "    model.to('cpu')\n",
    "    return {'nan_rate': nan/(cnndm_batch_size*total)}\n",
    "\n",
    "# Here we wrap the test function in a LineageTest class\n",
    "test = LineageTest(\n",
    "        eval_dataset=lineage_dataset,\n",
    "        metric_for_best_model='nan_rate',\n",
    "        custom_test_function=custom_test_function,\n",
    "        name='nan_rate_test',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading model: models/t5-base\n",
      "attempting to load model by specified task type\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 179/179 [00:45<00:00,  3.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unloading models/t5-base\n",
      "models/t5-base {'nan_rate': tensor(0.9285, device='cuda:0')}\n"
     ]
    }
   ],
   "source": [
    "# Here we run the test\n",
    "_, test_result = node.run_test(test, return_results=True) # result is stored in test_result\n",
    "node.unload_model(save_model=False) # its a good practice to save memory for other nodes\n",
    "print(node.output_dir, test_result) # print nan_rate with node name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. It's Your Turn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Now, its your turn to find models in the ```models``` directory that exhibit the bug, i.e. NaN rate > 0, and **report the NaN rate for each buggy model using the test function.**\n",
    "* Do not use multi-processing if you are writing any loops. \n",
    "* Interrupt the notebook when the instructor tells you to do so. \n",
    "* You may refer back to the tutorial for API usage.\n",
    "* #### Let the instructor know when you read this sentence.\n",
    "* #### Important Tip: It is impossible to test all models based on the time left, so you may want to think carefully about which models you want to test so that you can find as many buggy models as possible in the time limit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
