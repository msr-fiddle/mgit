{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment A (Group A): \n",
    "* In this assignment, you will first learn about a bug in a language model and how to reproduce it.\n",
    "* Given a pool of models, your goal is then to find **as many models as possible** that exhibit this bug, under a 15-minutes time constraint.\n",
    "* This notebook walks you through this process step-by-step. Run each cell of code and read the text instructions untill you read section 5 where you will need to write your own code to find the buggy models.\n",
    "* If you have any question during the assignment, please ask the instructor directly. It is prohibited to consult with any generative language models, e.g. ChatGPT, about this assignment. Please do not search for these bugs on the internet either."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### You are given 15 minutes to finish this assignment. Let the instructor start timing when you read this sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1: Library Import (run the code, no need to read through it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n",
    "os.environ['HF_HOME'] = '/workspace/HF_cache/'\n",
    "os.environ['HF_DATASETS_CACHE'] = '/workspace/HF_cache/datasets'\n",
    "os.environ['TRANSFORMERS_CACHE'] = '/workspace/HF_cache/transformers_cache/'\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS']='0'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1' \n",
    "import torch\n",
    "import time\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "import transformers\n",
    "from transformers import AutoConfig, AutoModel, AutoTokenizer\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Models\n",
    "\n",
    "* The ```models``` directory has 91 language models. You can inspect them using ```!ls models``` later.\n",
    "```\n",
    "TehranNLP-org\t       bert-large-uncased-7        roberta-large-10\n",
    "albert-base-v2\t      bert-large-uncased-8        roberta-large-2\n",
    "aloxatel\t            bert-large-uncased-9        roberta-large-3\n",
    "bert-base-cased         deepset\t\t\t         roberta-large-4\n",
    "bert-base-uncased       distilbert-base-cased-0\t roberta-large-5\n",
    "bert-large-cased-0      distilbert-base-cased-1\t roberta-large-6\n",
    "bert-large-cased-1      distilbert-base-cased-10    roberta-large-7\n",
    "bert-large-cased-10     distilbert-base-cased-2\t roberta-large-8\n",
    "bert-large-cased-2      distilbert-base-cased-3\t roberta-large-9\n",
    "bert-large-cased-3      distilbert-base-cased-4\t roberta-large-mnli-0\n",
    "bert-large-cased-4      distilbert-base-cased-5\t roberta-large-mnli-1\n",
    "bert-large-cased-5      distilbert-base-cased-6\t roberta-large-mnli-10\n",
    "bert-large-cased-6      distilbert-base-cased-7\t roberta-large-mnli-2\n",
    "bert-large-cased-7      distilbert-base-cased-8\t roberta-large-mnli-3\n",
    "bert-large-cased-8      distilbert-base-cased-9\t roberta-large-mnli-4\n",
    "bert-large-cased-9      distilbert-base-uncased\t roberta-large-mnli-5\n",
    "bert-large-uncased-0    doc2query\t\t \t\t  roberta-large-mnli-6\n",
    "bert-large-uncased-1    ericRosello\t\t \t\troberta-large-mnli-7\n",
    "bert-large-uncased-10   google\t\t\t \t\t roberta-large-mnli-8\n",
    "bert-large-uncased-2    howey\t\t\t \t\t  roberta-large-mnli-9\n",
    "bert-large-uncased-3    prajjwal1\t\t \t\t  t5-base\n",
    "bert-large-uncased-4    roberta-base\t\t \t   textattack\n",
    "bert-large-uncased-5    roberta-large-0\t\t \ttwmkn9\n",
    "bert-large-uncased-6    roberta-large-1\t\t \tvennify\n",
    "```\n",
    "* Some folders contain sub-directories with more models. For example, ```models/deepset``` has multiple models within it (e.g.,  ```bert-base-uncased-squad2```  ```roberta-base-squad2```  ```roberta-large-squad2```)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Here is an example of loading a model ```models/t5-base``` to the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'models/t5-base'\n",
    "config = AutoConfig.from_pretrained(model_path)\n",
    "architecture = config.architectures[0]\n",
    "model = getattr(transformers, architecture).from_pretrained(model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the code to load a dataset. You do not need to understand the details here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "from functools import partial\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchtext.datasets import CNNDM\n",
    "\n",
    "cnndm_batch_size = 64\n",
    "cnndm_datapipe = CNNDM(split=\"test\")\n",
    "task = \"summarize\"\n",
    "\n",
    "def apply_prefix(task, x):\n",
    "    return f\"{task}: \" + x[0], x[1]\n",
    "\n",
    "cnndm_datapipe = cnndm_datapipe.map(partial(apply_prefix, task))\n",
    "cnndm_datapipe = cnndm_datapipe.batch(cnndm_batch_size)\n",
    "cnndm_datapipe = cnndm_datapipe.rows2columnar([\"article\", \"abstract\"])\n",
    "dataset = DataLoader(cnndm_datapipe, shuffle=False, batch_size=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Bug Behavior: The Model Outputs NaN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* If a model outputs NaN (Not a Number), it means the model contains a bug.\n",
    "* Your colleague finds that the ```models/t5-base``` model, when loaded in torch.fp16 format (using ```model.half()```), returns NaN in its output.\n",
    "* Your colleague wrote a test function to compute the NaN rate, i.e. percentage of outputs that contain NaN when running on the entire given dataset. The NaN rate for ```models/t5-base``` is greater than 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the code; you do not need to understand the exact details here\n",
    "\n",
    "#Here we define the test function\n",
    "def custom_test_function(model, dataset, tokenizer):\n",
    "    fixed_input_length = 128\n",
    "    model.half()\n",
    "    model.to(\"cuda:0\")\n",
    "    model.eval()\n",
    "    \n",
    "    nan = 0\n",
    "    j = 0\n",
    "    decoder_input_ids = torch.tensor([[tokenizer.pad_token_id for n in range(fixed_input_length)] for m in range(cnndm_batch_size)]).to(\"cuda:0\")\n",
    "    total = sum(1 for e in dataset) - 1 #drop last\n",
    "\n",
    "    for batch in tqdm(dataset, total=total):\n",
    "        input_text = batch[\"article\"]\n",
    "\n",
    "        if j == total:#drop last\n",
    "            break\n",
    "\n",
    "        inputs = tokenizer(input_text, max_length=fixed_input_length, padding=True, truncation=True, return_tensors=\"pt\").to(\"cuda:0\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            try:\n",
    "                out = model(**inputs, decoder_input_ids=decoder_input_ids)#t5s are encoder+decoder\n",
    "            except Exception as e:\n",
    "                out = model(**inputs)\n",
    "                \n",
    "        try:\n",
    "            if hasattr(out, 'last_hidden_state'):\n",
    "                nan += sum([torch.isnan(out_).any() for out_ in out.last_hidden_state])\n",
    "            elif hasattr(out, 'logits'):\n",
    "                nan += sum([torch.isnan(out_).any() for out_ in out.logits])\n",
    "            else:\n",
    "                nan += sum([torch.isnan(out.start_logits[i]).any() and torch.isnan(out.end_logits[i]).any()\n",
    "                            for i in range(len(out.start_logits))])\n",
    "        except Exception as e:\n",
    "            print(e, \"model output interpretation is unsuccessful!\")\n",
    "            model.to('cpu')\n",
    "            return {'nan_rate': nan/(cnndm_batch_size*total)}\n",
    "        \n",
    "        j += 1\n",
    "\n",
    "    model.to('cpu')\n",
    "    return {'nan_rate': nan/(cnndm_batch_size*total)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 179/179 [00:20<00:00,  8.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/t5-base {'nan_rate': tensor(0.9285, device='cuda:0')}\n"
     ]
    }
   ],
   "source": [
    "# Here we run the test\n",
    "print(model_path, custom_test_function(model, dataset, tokenizer)) # print nan_rate with node name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. It's Your Turn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Now, its your turn to find models in the ```models``` directory that exhibit the bug, i.e. NaN rate > 0, and **report the NaN rate for each buggy model using the test function.**\n",
    "* Do not use multi-processing if you are writing any loops. \n",
    "* Interrupt the notebook when the instructor tells you to do so. \n",
    "* You may refer back to the tutorial for API usage.\n",
    "* #### Let the instructor know when you read this sentence.\n",
    "* #### Important Tip: It is impossible to test all models based on the time left, so you may want to think carefully about which models you want to test so that you can find as many buggy models as possible in the time limit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
