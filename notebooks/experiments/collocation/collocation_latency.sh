#python collocation.py --models /workspace/llms/meta-llama/Llama-2-7b-hf/ /workspace/llms/meta-llama/Llama-2-7b-chat-hf/ --mode collocate --profile_mode latency --collocate_percentage 0.30 --seq_len 32 --max_new_tokens 1| tee ./log/llama2_llama2chat_30_32_latency.log
#python collocation.py --models /workspace/llms/meta-llama/Llama-2-7b-hf/ /workspace/llms/meta-llama/Llama-2-7b-chat-hf/ --mode collocate --profile_mode latency --collocate_percentage 0.60 --seq_len 32 --max_new_tokens 1| tee ./log/llama2_llama2chat_60_32_latency.log
python collocation.py --models /workspace/llms/meta-llama/Llama-2-7b-hf/ /workspace/llms/meta-llama/Llama-2-7b-chat-hf/ --mode collocate --profile_mode latency --collocate_percentage 0.90 --seq_len 32 --max_new_tokens 1| tee ./log/llama2_llama2chat_90_32_latency.log
#
#python collocation.py --models /workspace/llms/meta-llama/Llama-2-7b-hf/ /workspace/llms/meta-llama/Llama-2-7b-chat-hf/ --mode collocate --profile_mode latency --collocate_percentage 0.30 --seq_len 64 --max_new_tokens 1| tee ./log/llama2_llama2chat_30_64_latency.log
#python collocation.py --models /workspace/llms/meta-llama/Llama-2-7b-hf/ /workspace/llms/meta-llama/Llama-2-7b-chat-hf/ --mode collocate --profile_mode latency --collocate_percentage 0.60 --seq_len 64 --max_new_tokens 1| tee ./log/llama2_llama2chat_60_64_latency.log
#python collocation.py --models /workspace/llms/meta-llama/Llama-2-7b-hf/ /workspace/llms/meta-llama/Llama-2-7b-chat-hf/ --mode collocate --profile_mode latency --collocate_percentage 0.90 --seq_len 64 --max_new_tokens 1| tee ./log/llama2_llama2chat_90_64_latency.log
#
#python collocation.py --models /workspace/llms/meta-llama/Llama-2-7b-hf/ /workspace/llms/meta-llama/Llama-2-7b-chat-hf/ --mode collocate --profile_mode latency --collocate_percentage 0.30 --seq_len 128 --max_new_tokens 1| tee ./log/llama2_llama2chat_30_128_latency.log
#python collocation.py --models /workspace/llms/meta-llama/Llama-2-7b-hf/ /workspace/llms/meta-llama/Llama-2-7b-chat-hf/ --mode collocate --profile_mode latency --collocate_percentage 0.60 --seq_len 128 --max_new_tokens 1| tee ./log/llama2_llama2chat_60_128_latency.log
#python collocation.py --models /workspace/llms/meta-llama/Llama-2-7b-hf/ /workspace/llms/meta-llama/Llama-2-7b-chat-hf/ --mode collocate --profile_mode latency --collocate_percentage 0.90 --seq_len 128 --max_new_tokens 1| tee ./log/llama2_llama2chat_90_128_latency.log
#
#python collocation.py --models /workspace/llms/meta-llama/Llama-2-7b-hf/ /workspace/llms/meta-llama/Llama-2-7b-chat-hf/ --mode collocate --profile_mode latency --collocate_percentage 0.30 --seq_len 256 --max_new_tokens 1| tee ./log/llama2_llama2chat_30_256_latency.log
#python collocation.py --models /workspace/llms/meta-llama/Llama-2-7b-hf/ /workspace/llms/meta-llama/Llama-2-7b-chat-hf/ --mode collocate --profile_mode latency --collocate_percentage 0.60 --seq_len 256 --max_new_tokens 1| tee ./log/llama2_llama2chat_60_256_latency.log
#python collocation.py --models /workspace/llms/meta-llama/Llama-2-7b-hf/ /workspace/llms/meta-llama/Llama-2-7b-chat-hf/ --mode collocate --profile_mode latency --collocate_percentage 0.90 --seq_len 256 --max_new_tokens 1| tee ./log/llama2_llama2chat_90_256_latency.log
